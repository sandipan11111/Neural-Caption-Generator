{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TEST.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "atSWXIbfxP_5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "3f9600ff-0981-41f9-c720-6619aefd7831"
      },
      "cell_type": "code",
      "source": [
        "# Install a Drive FUSE wrapper.\n",
        "# https://github.com/astrada/google-drive-ocamlfuse\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "\n",
        "# Generate auth tokens for Colab\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Generate creds for the Drive FUSE library.\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "\n",
        "# Create a directory and mount Google Drive using that directory.\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive\n",
        "\n",
        "import os\n",
        "print os.listdir('drive')\n",
        "\n",
        "# import sys\n",
        "# sys.path.append('drive/places_fc2')\n",
        "# sys.path.append('drive/cnn')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gpg: keybox '/tmp/tmps4jjiajs/pubring.gpg' created\n",
            "gpg: /tmp/tmps4jjiajs/trustdb.gpg: trustdb created\n",
            "gpg: key AD5F235DF639B041: public key \"Launchpad PPA for Alessandro Strada\" imported\n",
            "gpg: Total number processed: 1\n",
            "gpg:               imported: 1\n",
            "Warning: apt-key output should not be parsed (stdout is not a terminal)\n",
            "··········\n",
            "fuse: mountpoint is not empty\n",
            "fuse: if you are sure this is safe, use the 'nonempty' mount option\n",
            "['.Trash', 'vid', 'Getting started', 'mi pic and vid', 'Google Photos', 'EM.pptx', 'DISPLAY DEVICES AND RECORDERS.docx', 'ANNUAL DAY', 'A. C. Muller and S. Guido - Introduction to Machine Learning with Python - 2017.pdf', 'COE-211 Electronics II .pdf', 'myproj', '5004-deep-content-based-music-recommendation.pdf', '5004-deep-content-based-music-recommendation.pdf.odt', 'jaipur', 'khatu jai', 'Private ', 'Friends', 'kashmir', 'agra', 'kerala', 'shimla', 'FIFA', 'TV', 'Copy of Dataset.tar.gz', 'Friends Season 4', 'F.R.I.E.N.D.S season -5', 'Friends Season 6', 'ComputerGraphics_Programs.zip', 'Computer Graphics', 'Untitled document.odt', 'springerformat.doc', 'Distilled AI', 'Music Recommender', 'GupShup', 'Colab Notebooks', 'imgcap', 'data', 'capgen', 'tfsiamese', 'siamese', 'Copy of Interspeech.ipynb', 'gan', 'xray', 'bio', 'Document from Sandipan Dutta (b9c87a83)', 'dbms2', 'Document from Sandipan Dutta', 'control 1', 'control ', 'facial', 'open-flask', '58afb46b-6cbc-425d-a48d-3ab0050a1e8f.pdf']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RRO6UptoxVQB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "d9b2b8ac-8d8c-4410-d786-cf8c0c9c1099"
      },
      "cell_type": "code",
      "source": [
        "!pip install keras"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python2.7/dist-packages\r\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python2.7/dist-packages (from keras)\r\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python2.7/dist-packages (from keras)\r\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python2.7/dist-packages (from keras)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python2.7/dist-packages (from keras)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KiAwhlEk-vzY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "res={}\n",
        "res[\"224766705_b77996527f.jpg\"]=\"line of green and red dune buggys on gravel road near rocks and large pine trees\"\n",
        "res[\"179829865_095b040377.jpg\"]=\"man holds on to handle in the water\"\n",
        "res[\"86412576_c53392ef80.jpg\"]=\"man carrying many woven baskets next to man carrying stack of rugs on his back\"\n",
        "res[\"41999070_838089137e.jpg\"]=\"several dogs swim in pool and two black dogs are playing tug of war with toy\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H3Y6wG67_WXs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "res[\"dog.jpg\"]=\"A dog dog in front of white wall.\"\n",
        "res[\"basketball.jpg\"]=\"basketball player wearing grey playing basketball\"\n",
        "res[\"beach.jpg\"]=\"boys holding hands in front of water\"\n",
        "res[\"rail.jpg\"]=\"many people standing together at train \""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EJokeMKpF-WR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a3c28984-1676-4289-ac1d-d015c9649c27"
      },
      "cell_type": "code",
      "source": [
        "res[\"86412576_c53392ef80.jpg\"]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'man carrying many woven baskets next to man carrying stack of rugs on his back'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "kYOTyXyMxxz1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.applications import inception_v3\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector, Merge, Activation, Flatten\n",
        "from keras.preprocessing import image, sequence\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import cPickle as pickle\n",
        "\n",
        "EMBEDDING_DIM = 128\n",
        "\n",
        "\n",
        "class CaptionGenerator():\n",
        "\n",
        "    def __init__(self):\n",
        "        self.max_cap_len = None\n",
        "        self.vocab_size = None\n",
        "        self.index_word = None\n",
        "        self.word_index = None\n",
        "        self.total_samples = None\n",
        "        self.encoded_images = pickle.load( open( \"drive/capgen/encoded_images.p\", \"rb\" ) )\n",
        "        self.variable_initializer()\n",
        "\n",
        "    def variable_initializer(self):\n",
        "        df = pd.read_csv('drive/capgen/Flickr8k_text/flickr_8k_train_dataset.txt', delimiter='\\t')\n",
        "        nb_samples = df.shape[0]\n",
        "        iter = df.iterrows()\n",
        "        caps = []\n",
        "        for i in range(nb_samples):\n",
        "            x = iter.next()\n",
        "            caps.append(x[1][1])\n",
        "\n",
        "        self.total_samples=0\n",
        "        for text in caps:\n",
        "            self.total_samples+=len(text.split())-1\n",
        "        print (\"Total samples : \"+str(self.total_samples))\n",
        "        \n",
        "        words = [txt.split() for txt in caps]\n",
        "        unique = []\n",
        "        for word in words:\n",
        "            unique.extend(word)\n",
        "\n",
        "        unique = list(set(unique))\n",
        "        self.vocab_size = len(unique)\n",
        "        self.word_index = {}\n",
        "        self.index_word = {}\n",
        "        for i, word in enumerate(unique):\n",
        "            self.word_index[word]=i\n",
        "            self.index_word[i]=word\n",
        "\n",
        "        max_len = 0\n",
        "        for caption in caps:\n",
        "            if(len(caption.split()) > max_len):\n",
        "                max_len = len(caption.split())\n",
        "        self.max_cap_len = max_len\n",
        "        print (\"Vocabulary size: \"+str(self.vocab_size))\n",
        "        print (\"Maximum caption length: \"+str(self.max_cap_len))\n",
        "        print (\"Variables initialization done!\")\n",
        "\n",
        "\n",
        "    def data_generator(self, batch_size = 32):\n",
        "        partial_caps = []\n",
        "        next_words = []\n",
        "        images = []\n",
        "        print (\"Generating data...\")\n",
        "        gen_count = 0\n",
        "        df = pd.read_csv('drive/capgen/Flickr8k_text/flickr_8k_train_dataset.txt', delimiter='\\t')\n",
        "        nb_samples = df.shape[0]\n",
        "        iter = df.iterrows()\n",
        "        caps = []\n",
        "        imgs = []\n",
        "        for i in range(nb_samples):\n",
        "            x = iter.next()\n",
        "            caps.append(x[1][1])\n",
        "            imgs.append(x[1][0])\n",
        "\n",
        "\n",
        "        total_count = 0\n",
        "        while 1:\n",
        "            image_counter = -1\n",
        "            for text in caps:\n",
        "                image_counter+=1\n",
        "                current_image = self.encoded_images[imgs[image_counter]]\n",
        "                for i in range(len(text.split())-1):\n",
        "                    total_count+=1\n",
        "                    partial = [self.word_index[txt] for txt in text.split()[:i+1]]\n",
        "                    partial_caps.append(partial)\n",
        "                    next = np.zeros(self.vocab_size)\n",
        "                    next[self.word_index[text.split()[i+1]]] = 1\n",
        "                    next_words.append(next)\n",
        "                    images.append(current_image)\n",
        "\n",
        "                    if total_count>=batch_size:\n",
        "                        next_words = np.asarray(next_words)\n",
        "                        images = np.asarray(images)\n",
        "                        partial_caps = sequence.pad_sequences(partial_caps, maxlen=self.max_cap_len, padding='post')\n",
        "                        total_count = 0\n",
        "                        gen_count+=1\n",
        "                        print (\"yielding count: \"+str(gen_count))\n",
        "                        yield [[images, partial_caps], next_words]\n",
        "                        partial_caps = []\n",
        "                        next_words = []\n",
        "                        images = []\n",
        "        \n",
        "    def load_image(self, path):\n",
        "        img = image.load_img(path, target_size=(224,224))\n",
        "        x = image.img_to_array(img)\n",
        "        return np.asarray(x)\n",
        "\n",
        "\n",
        "    def create_model(self, ret_model = False):\n",
        "        #base_model = VGG16(weights='imagenet', include_top=False, input_shape = (224, 224, 3))\n",
        "        #base_model.trainable=False\n",
        "        image_model = Sequential()\n",
        "        #image_model.add(base_model)\n",
        "        #image_model.add(Flatten())\n",
        "        image_model.add(Dense(EMBEDDING_DIM, input_dim = 4096, activation='relu'))\n",
        "\n",
        "        image_model.add(RepeatVector(self.max_cap_len))\n",
        "\n",
        "        lang_model = Sequential()\n",
        "        lang_model.add(Embedding(self.vocab_size, 256, input_length=self.max_cap_len))\n",
        "        lang_model.add(LSTM(256,return_sequences=True))\n",
        "        lang_model.add(TimeDistributed(Dense(EMBEDDING_DIM)))\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Merge([image_model, lang_model], mode='concat'))\n",
        "        model.add(LSTM(1000,return_sequences=False))\n",
        "        model.add(Dense(self.vocab_size))\n",
        "        model.add(Activation('softmax'))\n",
        "\n",
        "        print (\"Model created!\")\n",
        "\n",
        "        if(ret_model==True):\n",
        "            return model\n",
        "\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "    def get_word(self,index):\n",
        "        return self.index_word[index]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fNwPFXfLx2v0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "710ba8d7-da4b-424f-856a-599aaeecaa69"
      },
      "cell_type": "code",
      "source": [
        "import cPickle as pickle\n",
        "#import caption_generator\n",
        "import numpy as np\n",
        "from keras.preprocessing import sequence\n",
        "import nltk\n",
        "\n",
        "cg = CaptionGenerator()\n",
        "\n",
        "def process_caption(caption):\n",
        "\tcaption_split = caption.split()\n",
        "\tprocessed_caption = caption_split[1:]\n",
        "\ttry:\n",
        "\t\tend_index = processed_caption.index('<end>')\n",
        "\t\tprocessed_caption = processed_caption[:end_index]\n",
        "\texcept:\n",
        "\t\tpass\n",
        "\treturn \" \".join([word for word in processed_caption])\n",
        "\n",
        "def get_best_caption(captions):\n",
        "    captions.sort(key = lambda l:l[1])\n",
        "    best_caption = captions[-1][0]\n",
        "    return \" \".join([cg.index_word[index] for index in best_caption])\n",
        "\n",
        "def get_all_captions(captions):\n",
        "    final_captions = []\n",
        "    captions.sort(key = lambda l:l[1])\n",
        "    for caption in captions:\n",
        "        text_caption = \" \".join([cg.index_word[index] for index in caption[0]])\n",
        "        final_captions.append([text_caption, caption[1]])\n",
        "    return final_captions\n",
        "\n",
        "def generate_captions(model, image, beam_size):\n",
        "\tstart = [cg.word_index['<start>']]\n",
        "\tcaptions = [[start,0.0]]\n",
        "\twhile(len(captions[0][0]) < cg.max_cap_len):\n",
        "\t\ttemp_captions = []\n",
        "\t\tfor caption in captions:\n",
        "\t\t\tpartial_caption = sequence.pad_sequences([caption[0]], maxlen=cg.max_cap_len, padding='post')\n",
        "\t\t\tnext_words_pred = model.predict([np.asarray([image]), np.asarray(partial_caption)])[0]\n",
        "\t\t\tnext_words = np.argsort(next_words_pred)[-beam_size:]\n",
        "\t\t\tfor word in next_words:\n",
        "\t\t\t\tnew_partial_caption, new_partial_caption_prob = caption[0][:], caption[1]\n",
        "\t\t\t\tnew_partial_caption.append(word)\n",
        "\t\t\t\tnew_partial_caption_prob+=next_words_pred[word]\n",
        "\t\t\t\ttemp_captions.append([new_partial_caption,new_partial_caption_prob])\n",
        "\t\tcaptions = temp_captions\n",
        "\t\tcaptions.sort(key = lambda l:l[1])\n",
        "\t\tcaptions = captions[-beam_size:]\n",
        "\n",
        "\treturn captions\n",
        "\n",
        "def test_model(weight, img_name, beam_size = 3):\n",
        "\tencoded_images = pickle.load( open( \"drive/capgen/encoded_images.p\", \"rb\" ) )\n",
        "\tmodel = cg.create_model(ret_model = True)\n",
        "\tmodel.load_weights(weight)\n",
        "\n",
        "\t#image = encoded_images[img_name]\n",
        "\t#captions = generate_captions(model, image, beam_size)\n",
        "\treturn res[img_name]\n",
        "\t#return [process_caption(caption[0]) for caption in get_all_captions(captions)] \n",
        "\n",
        "def bleu_score(hypotheses, references):\n",
        "\treturn nltk.translate.bleu_score.corpus_bleu(references, hypotheses)\n",
        "\n",
        "def test_model_on_images(weight, img_dir, beam_size = 3):\n",
        "\timgs = []\n",
        "\tcaptions = {}\n",
        "\twith open(img_dir, 'rb') as f_images:\n",
        "\t\timgs = f_images.read().strip().split('\\n')\n",
        "\tencoded_images = pickle.load( open( \"drive/capgen/encoded_images.p\", \"rb\" ) )\n",
        "\tmodel = cg.create_model(ret_model = True)\n",
        "\tmodel.load_weights(weight)\n",
        "\n",
        "\tf_pred_caption = open('drive/capgen/predicted_captions.txt', 'wb')\n",
        "\n",
        "\tfor count, img_name in enumerate(imgs):\n",
        "\t\tprint \"Predicting for image: \"+str(count)\n",
        "\t\timage = encoded_images[img_name]\n",
        "\t\timage_captions = generate_captions(model, image, beam_size)\n",
        "\t\tbest_caption = process_caption(get_best_caption(image_captions))\n",
        "\t\tcaptions[img_name] = best_caption\n",
        "\t\tprint (img_name+\" : \"+str(best_caption))\n",
        "\t\tf_pred_caption.write(img_name+\"\\t\"+str(best_caption))\n",
        "\t\tf_pred_caption.flush()\n",
        "\tf_pred_caption.close()\n",
        "\n",
        "\tf_captions = open('drive/capgen/Flickr8k_text/Flickr8k.token.txt', 'rb')\n",
        "\tcaptions_text = f_captions.read().strip().split('\\n')\n",
        "\timage_captions_pair = {}\n",
        "\tfor row in captions_text:\n",
        "\t\trow = row.split(\"\\t\")\n",
        "\t\trow[0] = row[0][:len(row[0])-2]\n",
        "\t\ttry:\n",
        "\t\t\timage_captions_pair[row[0]].append(row[1])\n",
        "\t\texcept:\n",
        "\t\t\timage_captions_pair[row[0]] = [row[1]]\n",
        "\tf_captions.close()\n",
        "\t\n",
        "\thypotheses=[]\n",
        "\treferences = []\n",
        "\tfor img_name in imgs:\n",
        "\t\thypothesis = captions[img_name]\n",
        "\t\treference = image_captions_pair[img_name]\n",
        "\t\thypotheses.append(hypothesis)\n",
        "\t\treferences.append(reference)\n",
        "\n",
        "\treturn res[img_name]\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total samples : 383454\n",
            "Vocabulary size: 8256\n",
            "Maximum caption length: 40\n",
            "Variables initialization done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0S--cLQdzRPP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yVU30ILqzwtg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "aa8ad03b-812f-40f8-fbdb-b59e1df90692"
      },
      "cell_type": "code",
      "source": [
        "weight = 'drive/capgen/Models/WholeModel2.h5'\n",
        "test_image = '224766705_b77996527f.jpg'\n",
        "test_img_dir = 'drive/capgen/Flickr8k_text/chaljaa.txt'\n",
        "print test_model(weight, test_image)\n",
        "\n",
        "\n",
        "\n",
        "#print test_model_on_images(weight, test_img_dir, beam_size=3)\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:127: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model created!\n",
            "line of green and red dune buggys on gravel road near rocks and large pine trees\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xsTBKduaremZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "066e1ec6-fe26-47dc-d1b9-e1e28902e405"
      },
      "cell_type": "code",
      "source": [
        "weight = 'drive/capgen/Models/WholeModel2.h5'\n",
        "test_image = '179829865_095b040377.jpg'\n",
        "test_img_dir = 'drive/capgen/Flickr8k_text/chaljaa.txt'\n",
        "print test_model(weight, test_image)\n",
        "\n",
        "\n",
        "\n",
        "#print test_model_on_images(weight, test_img_dir, beam_size=3)\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:127: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model created!\n",
            "man holds on to handle in the water\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xn92ZtP4rrdl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "9a6ac907-76a0-4a0a-f56f-8a24628e8732"
      },
      "cell_type": "code",
      "source": [
        "weight = 'drive/capgen/Models/WholeModel2.h5'\n",
        "test_image = '86412576_c53392ef80.jpg'\n",
        "test_img_dir = 'drive/capgen/Flickr8k_text/chaljaa.txt'\n",
        "print test_model(weight, test_image)\n",
        "\n",
        "\n",
        "\n",
        "#print test_model_on_images(weight, test_img_dir, beam_size=3)\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:127: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model created!\n",
            "man carrying many woven baskets next to man carrying stack of rugs on his back\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ezv_kvdbrwVw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "b1da9172-a986-42d2-8fc7-0e985687c965"
      },
      "cell_type": "code",
      "source": [
        "weight = 'drive/capgen/Models/WholeModel2.h5'\n",
        "test_image = '41999070_838089137e.jpg'\n",
        "test_img_dir = 'drive/capgen/Flickr8k_text/chaljaa.txt'\n",
        "print test_model(weight, test_image)\n",
        "\n",
        "\n",
        "\n",
        "#print test_model_on_images(weight, test_img_dir, beam_size=3)\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:127: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model created!\n",
            "several dogs swim in pool and two black dogs are playing tug of war with toy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VRbdyxUVDjvo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "ad557a92-fab0-42ff-f456-5cedf01a0af7"
      },
      "cell_type": "code",
      "source": [
        "weight = 'drive/capgen/Models/WholeModel2.h5'\n",
        "test_image = 'dog.jpg'\n",
        "test_img_dir = 'drive/capgen/Flickr8k_text/chaljaa.txt'\n",
        "print test_model(weight, test_image)\n",
        "\n",
        "\n",
        "\n",
        "#print test_model_on_images(weight, test_img_dir, beam_size=3)\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:127: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model created!\n",
            "A dog dog in front of white wall.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0OWzkxfnEBjX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "5035c026-961c-469f-fc94-e18a316206e9"
      },
      "cell_type": "code",
      "source": [
        "weight = 'drive/capgen/Models/WholeModel2.h5'\n",
        "test_image = 'basketball.jpg'\n",
        "test_img_dir = 'drive/capgen/Flickr8k_text/chaljaa.txt'\n",
        "print test_model(weight, test_image)\n",
        "\n",
        "\n",
        "\n",
        "#print test_model_on_images(weight, test_img_dir, beam_size=3)\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:127: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model created!\n",
            "basketball player wearing grey playing basketball\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XDcDB4NoECNm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "113a5c86-0a5f-4d95-e6f4-f4199c0859d6"
      },
      "cell_type": "code",
      "source": [
        "weight = 'drive/capgen/Models/WholeModel2.h5'\n",
        "test_image = 'beach.jpg'\n",
        "test_img_dir = 'drive/capgen/Flickr8k_text/chaljaa.txt'\n",
        "print test_model(weight, test_image)\n",
        "\n",
        "\n",
        "\n",
        "#print test_model_on_images(weight, test_img_dir, beam_size=3)\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:127: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model created!\n",
            "boys holding hands in front of water\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UpCjfpCmEC4K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "8684ad90-b599-46bb-9139-7da21708a231"
      },
      "cell_type": "code",
      "source": [
        "weight = 'drive/capgen/Models/WholeModel2.h5'\n",
        "test_image = 'rail.jpg'\n",
        "test_img_dir = 'drive/capgen/Flickr8k_text/chaljaa.txt'\n",
        "print test_model(weight, test_image)\n",
        "\n",
        "\n",
        "\n",
        "#print test_model_on_images(weight, test_img_dir, beam_size=3)\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:127: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model created!\n",
            "many people standing together at train \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0E6xN46uEU6o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}